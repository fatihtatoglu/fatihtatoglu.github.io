---
id: ummxbsvyq0
lang: en
title: "AI Debate Experiment: Thinking, Persuasion, and Skills for the Future"
slug: debate-experiment-with-ai
category: life-learning
tags:
  - ai-debate
  - artificial-intelligence
  - analytical-thinking
  - persuasion
  - future-of-jobs
  - human-skills
  - decision-making
  - experimental-learning
  - critical-thinking
readingTime: 13
date: 2026-01-06
updated: 2026-01-06
pair: munazara-deneyi-yapay-zeka
canonical: ~/en/debate-experiment-with-ai/
alternate: ~/munazara-deneyi-yapay-zeka/
description: "A four-round AI debate experiment inspired by the Future of Jobs 2025, exploring analytical thinking, persuasion, and human skills in the age of AI."
keywords:
  - "ai debate"
  - "analytical thinking"
  - "persuasion"
  - "future of jobs"
  - "human skills"
featured: true
cover: /assets/images/thinking-speaking-persuading.webp
coverAlt: A formal debate scene with two humanoid robots facing each other at podiums and a gavel symbolizing the jury between them.
coverCaption: "A symbolic scene representing an AI vs AI debate focused on argumentation, structure, and persuasion."
template: post
layout: default
status: published
aiTranslated: ChatGPT 5.2
---
# AI Debate Experiment: Thinking, Persuasion, and Skills for the Future

In a few AI seminars I attended in 2025, the most common question was "Will artificial intelligence take our jobs?" What caught my attention, though, was something else: how I could make myself more resilient and more useful in a world shaped by AI. With that in mind, I thought it would be helpful to look at the World Economic Forum's Future of Jobs Report 2025. In this post, I will briefly talk about this report, and then, based on some of its points, share an experiment idea.

The reason I am writing this is not to talk about these skills in theory. I wanted to actually exercise them through a small, measurable, and repeatable experiment. The debate experiment I describe below came out of this curiosity. I chose to read the report not as a roadmap, but as raw material that can be tested and experimented on.

This is not a technical paper, nor a benchmark of AI models. It is a personal learning experiment: a way to use artificial intelligence as a structured mirror for thinking, arguing, and persuasion skills that are increasingly hard to practice in everyday work.

## WEF Future of Jobs Report 2025 and Key Skills

This report, prepared by the World Economic Forum, aims to identify opportunities and risks by analyzing the transformation of the global labor market between 2025 and 2030. It includes data-based insights on how topics like technological progress, economic fluctuations, and social expectations are likely to affect the business world.

In simpler terms, this report is like a modern nautical map and weather forecast for ships sailing through a fast-changing and sometimes stormy sea. The map (the report) shows approaching storms (declining jobs and economic crises) and newly discovered islands (growing sectors and new skills), helping captains (leaders and individuals) adjust their routes toward a safer and more profitable future.

According to the report, there are several technical, social, behavioral, and personal skills recommended to be gained between 2025 and 2030. The report predicts that 39 percent of existing skills will either disappear or transform, and it highlights that the skills listed below will become more valuable for survival in the business world. Most of these skills are not learned in isolation but developed through practice. What I realized was this: these skills are hard to measure, but possible to train.

**Technical Skills**

- Artificial Intelligence and Big Data
- Networks and Cybersecurity
- Technology Literacy
- Environmental Management

**Social and Personal Skills**

- Analytical Thinking
- Resilience, Flexibility, and Agility
- Creative Thinking
- Leadership and Social Influence
- Curiosity and Lifelong Learning
- Motivation and Self-Awareness
- Empathy and Active Listening
- Talent Management, Teaching, and Mentoring

While reading the report, I noticed an uncomfortable pattern. Most of the skills I already felt confident about were technical. The ones I struggled to assess in myself were the human ones: thinking clearly under pressure, defending ideas, changing my mind without ego. That realization was one of the quiet triggers behind this experiment.

## Debate: A Tool for Analytical Thinking and Persuasion

I could write a lot about the outcomes of this report, and I have many different ideas in mind. But in this post, I want to focus on "debate." The debate format I once looked down on in high school, and only watched as a spectator during university competitions, actually covers several of the skills listed in the report.

A debate is a form of discussion where opposing views on a specific topic are defended within defined rules. The goal is not to decide who is right, but to defend arguments with reasoning and try to challenge opposing arguments through logical thinking. You can think of it as a kind of mental workout. You structure your thoughts or the position you defend and try to persuade the other side and the audience.

Practicing debate helps build a way of thinking, spot weak or blind points, and develop analytical thinking. On top of that, it improves expression, active listening, and the ability to clearly explain yourself. In other words, it shows that some of the skills predicted by the report can be gained simply by learning how to debate.

The reason I chose artificial intelligence here is that, in debates with people, we often remember the tone rather than the argument itself. I wanted to remove tone completely. With the same rules and the same boundaries, I aimed to measure only the argument itself. This helped me focus on "what is being said" rather than "who is saying it."

## Debate Experiment with Artificial Intelligence: Purpose and Approach

What I wanted to test with this experiment was not which AI model is better, but how effective a structured debate format is in measuring and producing quality thinking.

There are many areas where I have gotten my hands dirty with AI, but due to technical limitations, some of them are moving slowly. Still, within my current means, I mostly just need an excuse to bring some experiments to life. This report turned out to be a great excuse for this particular experiment.

The goal of the experiment is to let two different AI models, with similar quality in terms of parameters and training data, talk and argue with each other. Then, I planned to share the outputs with a few people, have them score the results, and review the outputs with human judgment to obtain high-quality training data. I thought the best way to create such a discussion environment would be a debate competition.

> By the way, there might already be people or teams doing similar work. I did not come across any during my research, but I am intentionally avoiding saying that I am the first. If you know of such work, please share it with me so I can learn and review their outputs as well.

## Rules and Structure of the Debate Experiment

While developing the idea further, I started looking into what topics people debate most and under which rules. From what I read and the competition videos I watched, I saw that although debates have very simple rules, there is serious strategic work happening behind the scenes. I also noticed that there are certain standard techniques.

As a result of my research, and also to validate the experiment, I decided on a four-round debate flow around a single main topic.

In the first round, the teams present only their own position and the arguments supporting it. They do not respond to the other side at all. In the second round, the sides focus on what the opponent said and explain why they disagree, trying to challenge the opposing arguments. The third round is similar to the second, but this time the focus is on why the assumptions of the other side are flawed. In the final round, I ask the sides to wrap up their views and explain why they still defend their position, or why they now agree with the opposing view, based on the counterarguments presented.

Although this flow is very similar to human debates, I could not set time limits for AI models. Instead, I chose to set word and argument limits. In each round, the models can use up to 500 words and 3 arguments. The response rounds have similar limits. These constraints were also meant to prevent the models from producing long but empty answers. The goal was not volume, but density.

For the first experiment, I chose a topic that we all experienced during the pandemic and that is still used as an excuse for many mass layoffs today: _working remotely is more productive than working from the office_. Many articles and studies have been written on this topic, so I was curious to see what the AI models would produce using this information.

## How Were the Debate Outputs Evaluated?

After defining the rules and the topic, the most critical part was evaluating the outputs in an unbiased way. I thought the evaluation could be based on conceptual clarity, logical consistency, strength of arguments, quality of counterarguments, practical realism, and summarization and inference skills. A winner would be decided based on the total score across these six categories, and I would also share this result with the AI models. I intentionally kept the win-lose setup, because competition, whether between humans or machines, seriously accelerates the production of ideas.

I chose these six criteria to measure both the quality of thinking and practical applicability together.

## Running the Experiment: Model Selection and Process

While running the experiment, I selected two strong AI models and, to avoid any imbalance, I used both of them through [Open Router](https://openrouter.ai/) with paid usage. While selecting the models, I relied on the usage trend charts provided by the platform.

I looked at the models with the highest token usage between December 15, 2025 and January 6, 2026 in the following categories:

- Technology
- Science
- Health
- Academia
- Legal
- Trivia
- Roleplay

From these lists, I selected the top two models. As seen in the table, this would have been Mimo V2 Flash and Gemini 2.5 Flash. However, since I encountered errors when sending requests to the Mimo V2 Flash model, I continued with DeepSeek V3.2 instead.

|                               | Technology | Science | Health | Academia | Trivia | Legal | Roleplay | Total |
|------------------------------:|:----------:|:-------:|:------:|:--------:|:------:|:-----:|:--------:|:-----:|
| Mimo V2 Flash                 | 3          | 9       | 0      | 8        | 12     | 0     | 3        | 35    |
| Gemini 2.5 Flash              | 2          | 4       | 8      | 8        | 0      | 3     | 2        | 27    |
| DeepSeek V3.2                 | 0          | 0       | 0      | 2        | 0      | 0     | 12       | 14    |
| Devstral 2 2512 (free)        | 0          | 0       | 3      | 3        | 0      | 7     | 0        | 13    |
| gpt-oss-120b                  | 0          | 4       | 0      | 0        | 0      | 8     | 0        | 12    |
| Gemini 3 Flash Preview        | 0          | 5       | 4      | 0        | 0      | 1     | 0        | 10    |
| Claude Sonnet 4.5             | 7          | 2       | 0      | 0        | 0      | 0     | 0        | 9     |
| Claude Haiku 4.5              | 2          | 0       | 6      | 0        | 0      | 0     | 0        | 8     |
| GPT-5.2                       | 2          | 0       | 0      | 3        | 1      | 0     | 0        | 6     |
| Claude Opus 4.5               | 5          | 0       | 0      | 0        | 0      | 0     | 0        | 5     |
| DeepSeek R1 T2 Chimera        | 0          | 0       | 0      | 0        | 0      | 0     | 5        | 5     |
| Gemini 2.5 Flash Lite         | 1          | 0       | 0      | 0        | 0      | 3     | 0        | 4     |
| GPT-4o-mini                   | 0          | 0       | 0      | 0        | 3      | 0     | 0        | 3     |
| Grok 4.1 Fast                 | 0          | 0       | 0      | 0        | 3      | 0     | 0        | 3     |
| Llama 3.1 70B Instruct        | 0          | 0       | 0      | 0        | 3      | 0     | 0        | 3     |
| MiniMax M2                    | 2          | 0       | 0      | 0        | 0      | 0     | 0        | 2     |
| Gemini 2.0 Flash              | 0          | 0       | 0      | 0        | 2      | 0     | 0        | 2     |
| Qwen3 235B A22B Instruct 2507 | 0          | 0       | 0      | 0        | 0      | 2     | 0        | 2     |
| DeepSeek V3 0324              | 0          | 0       | 0      | 0        | 0      | 0     | 2        | 2     |
| Claude Sonnet 4               | 0          | 0       | 1      | 0        | 0      | 0     | 0        | 1     |
| gpt-oss-20b                   | 0          | 0       | 1      | 0        | 0      | 0     | 0        | 1     |
| GPT-4.1 Nano                  | 0          | 0       | 1      | 0        | 0      | 0     | 0        | 1     |

For the first round, I will call the Gemini 2.5 Flash model A and the DeepSeek V3.2 model B. Model A will defend the position that _remote work is more productive_. Model B will defend the position that _working from the office is more productive_.

While running the experiment, DeepSeek V3.2 showed a bit of distraction, but it still produced the expected outputs.

I will also complete and share the code I used in this experiment soon.

## Results and Lessons from the Debate Experiment

The sides were AI, and when I also used AI to evaluate the results, the winner turned out to be model B. Model B won the competition by a margin of just 10 points. The total cost of this experiment was 0.010573 USD, which is about 0.46 TRY. I should also mention that I used GPT-5.2 as the jury model.

So what did I gain from this experiment? First of all, I practiced debate methods in a hands-on way. On top of that, I ended up with a template project, which means I can now run many other experiments using the same structure. Lastly, there is a very fast-moving AI train, and somehow we need to jump on it and hold on to at least one part of it. Otherwise, the cost can be quite high later.

So what is the role of AI here? This experiment showed me more clearly that AI is not an "answer machine" in this context, but more like a training partner. When you give it clear rules and a solid framework, it creates a space where you can repeatedly train your thinking, explaining, and persuading muscles.

The point is not for AI to think instead of us, but for us to set the right questions, boundaries, and playing fields so that we can think better. The debate format is just one of these fields, but perhaps one of the most disciplined and measurable ones.

This was only the first attempt. I plan to repeat the same format with different topics, different models, and even human participation. As the outputs change, the questions will also change. Maybe you can also support the experiment by suggesting debate topics.

In the end, the real question is not whether AI will surpass us, but which muscles we choose to strengthen alongside it, and whether we can shorten the list of "I wish I had" moments from the past with the help of AI.

## Sources and Further Reading

- [World Economic Forum – Future of Jobs Report 2025](https://www.weforum.org/stories/2025/01/future-of-jobs-report-2025-the-fastest-growing-and-declining-jobs/) – A comprehensive analysis of the future labor market, rising and declining roles, and the skills expected to stand out in the coming years.
- [Debating Guide - Durham Union Society](https://dus.org.uk/debating-guide/) – A practical guide on the basic rules of debate, argument construction, and developing counterarguments.